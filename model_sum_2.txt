================================================================================
FULL MODEL ARCHITECTURE
================================================================================

Florence2ForConditionalGeneration(
  (vision_tower): DaViT(
    (convs): ModuleList(
      (0): ConvEmbed(
        (proj): Conv2d(3, 128, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (1): ConvEmbed(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (2): ConvEmbed(
        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (3): ConvEmbed(
        (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (blocks): ModuleList(
      (0): MySequential(
        (0): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                )
              )
              (drop_path): Identity()
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (proj): Linear(in_features=128, out_features=128, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.004)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.004)
            )
          )
        )
      )
      (1): MySequential(
        (0): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.009)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.009)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.013)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.013)
            )
          )
        )
      )
      (2): MySequential(
        (0): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.017)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.017)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.022)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.022)
            )
          )
        )
        (1): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.026)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.026)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.030)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.030)
            )
          )
        )
        (2): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.035)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.035)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.039)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.039)
            )
          )
        )
        (3): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.043)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.043)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.048)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.048)
            )
          )
        )
        (4): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.052)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.052)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.057)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.057)
            )
          )
        )
        (5): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.061)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.061)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.065)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.065)
            )
          )
        )
        (6): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.070)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.070)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.074)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.074)
            )
          )
        )
        (7): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.078)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.078)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.083)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.083)
            )
          )
        )
        (8): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.087)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.087)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.091)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.091)
            )
          )
        )
      )
      (3): MySequential(
        (0): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.096)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.096)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.100)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.100)
            )
          )
        )
      )
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
  )
  (wordgrid_embeddings): WordnnEmbedding(
    (embedding): Embedding(30522, 768)
    (embedding_proj): Linear(in_features=768, out_features=64, bias=False)
  )
  (grid_tower): DaViT(
    (convs): ModuleList(
      (0): ConvEmbed(
        (proj): Conv2d(64, 128, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (1): ConvEmbed(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (2): ConvEmbed(
        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (3): ConvEmbed(
        (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (blocks): ModuleList(
      (0): MySequential(
        (0): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                )
              )
              (drop_path): Identity()
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (proj): Linear(in_features=128, out_features=128, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.004)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.004)
            )
          )
        )
      )
      (1): MySequential(
        (0): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.009)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.009)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.013)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.013)
            )
          )
        )
      )
      (2): MySequential(
        (0): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.017)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.017)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.022)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.022)
            )
          )
        )
        (1): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.026)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.026)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.030)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.030)
            )
          )
        )
        (2): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.035)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.035)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.039)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.039)
            )
          )
        )
        (3): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.043)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.043)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.048)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.048)
            )
          )
        )
        (4): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.052)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.052)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.057)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.057)
            )
          )
        )
        (5): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.061)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.061)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.065)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.065)
            )
          )
        )
        (6): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.070)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.070)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.074)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.074)
            )
          )
        )
        (7): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.078)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.078)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.083)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.083)
            )
          )
        )
        (8): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.087)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.087)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.091)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.091)
            )
          )
        )
      )
      (3): MySequential(
        (0): MySequential(
          (spatial_block): SpatialBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              )
            )
            (window_attn): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): WindowAttention(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.096)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.096)
            )
          )
          (channel_block): ChannelBlock(
            (conv1): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              )
            )
            (channel_attn): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): ChannelAttention(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (drop_path): DropPath(drop_prob=0.100)
            )
            (conv2): PreNorm(
              (fn): DepthWiseConv2d(
                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              )
            )
            (ffn): PreNorm(
              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (fn): Mlp(
                (net): Sequential(
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
              )
              (drop_path): DropPath(drop_prob=0.100)
            )
          )
        )
      )
    )
    (norms): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Linear(in_features=1024, out_features=1000, bias=True)
  )
  (image_proj_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (image_pos_embed): LearnedAbsolutePositionEmbedding2D(
    (row_embeddings): Embedding(50, 512)
    (column_embeddings): Embedding(50, 512)
  )
  (visual_temporal_embed): PositionalEmbeddingCosine1D()
  (language_model): Florence2LanguageForConditionalGeneration(
    (model): Florence2LanguageModel(
      (shared): Embedding(51289, 768, padding_idx=1)
      (encoder): Florence2Encoder(
        (embed_tokens): Florence2ScaledWordEmbedding(51289, 768, padding_idx=1)
        (embed_positions): Florence2LearnedPositionalEmbedding(1026, 768)
        (layers): ModuleList(
          (0-5): 6 x Florence2EncoderLayer(
            (self_attn): Florence2SdpaAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (activation_fn): GELUActivation()
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (decoder): Florence2Decoder(
        (embed_tokens): Florence2ScaledWordEmbedding(51289, 768, padding_idx=1)
        (embed_positions): Florence2LearnedPositionalEmbedding(1026, 768)
        (layers): ModuleList(
          (0-5): 6 x Florence2DecoderLayer(
            (self_attn): Florence2SdpaAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): GELUActivation()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): Florence2SdpaAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (lm_head): Linear(in_features=768, out_features=51289, bias=False)
  )
)


================================================================================
MODEL SUMMARY (using torchinfo.summary)
================================================================================

==========================================================================================
Layer (type:depth-idx)                                            Param #
==========================================================================================
Florence2ForConditionalGeneration                                 786,432
├─DaViT: 1-1                                                      --
│    └─ModuleList: 2-1                                            --
│    │    └─ConvEmbed: 3-1                                        19,200
│    │    └─ConvEmbed: 3-2                                        295,424
│    │    └─ConvEmbed: 3-3                                        1,180,672
│    │    └─ConvEmbed: 3-4                                        4,720,640
│    └─ModuleList: 2-2                                            --
│    │    └─MySequential: 3-5                                     401,664
│    │    └─MySequential: 3-6                                     1,589,760
│    │    └─MySequential: 3-7                                     56,927,232
│    │    └─MySequential: 3-8                                     25,233,408
│    └─AdaptiveAvgPool1d: 2-3                                     --
├─WordnnEmbedding: 1-2                                            --
│    └─Embedding: 2-4                                             (23,440,896)
│    └─Linear: 2-5                                                49,152
├─DaViT: 1-3                                                      --
│    └─ModuleList: 2-6                                            --
│    │    └─ConvEmbed: 3-9                                        401,792
│    │    └─ConvEmbed: 3-10                                       295,424
│    │    └─ConvEmbed: 3-11                                       1,180,672
│    │    └─ConvEmbed: 3-12                                       4,720,640
│    └─ModuleList: 2-7                                            --
│    │    └─MySequential: 3-13                                    401,664
│    │    └─MySequential: 3-14                                    1,589,760
│    │    └─MySequential: 3-15                                    56,927,232
│    │    └─MySequential: 3-16                                    25,233,408
│    └─LayerNorm: 2-8                                             2,048
│    └─AdaptiveAvgPool1d: 2-9                                     --
│    └─Linear: 2-10                                               1,025,000
├─LayerNorm: 1-4                                                  1,536
├─LearnedAbsolutePositionEmbedding2D: 1-5                         --
│    └─Embedding: 2-11                                            25,600
│    └─Embedding: 2-12                                            25,600
├─PositionalEmbeddingCosine1D: 1-6                                --
├─Florence2LanguageForConditionalGeneration: 1-7                  --
│    └─Florence2LanguageModel: 2-13                               --
│    │    └─Embedding: 3-17                                       39,389,952
│    │    └─Florence2Encoder: 3-18                                82,706,688
│    │    └─Florence2Decoder: 3-19                                96,890,112
│    └─Linear: 2-14                                               39,389,952
==========================================================================================
Total params: 464,851,560
Trainable params: 441,410,664
Non-trainable params: 23,440,896
==========================================================================================


================================================================================
TOP-LEVEL MODULES (named_children)
================================================================================

[vision_tower]
DaViT(
      (convs): ModuleList(
        (0): ConvEmbed(
          (proj): Conv2d(3, 128, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (1): ConvEmbed(
          (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): ConvEmbed(
          (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ConvEmbed(
          (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (blocks): ModuleList(
        (0): MySequential(
          (0): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=128, out_features=384, bias=True)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=128, out_features=512, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=512, out_features=128, bias=True)
                  )
                )
                (drop_path): Identity()
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=128, out_features=384, bias=True)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.004)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=128, out_features=512, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=512, out_features=128, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.004)
              )
            )
          )
        )
        (1): MySequential(
          (0): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=256, out_features=768, bias=True)
                  (proj): Linear(in_features=256, out_features=256, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.009)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.009)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=256, out_features=768, bias=True)
                  (proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.013)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.013)
              )
            )
          )
        )
        (2): MySequential(
          (0): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.017)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.017)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.022)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.022)
              )
            )
          )
          (1): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.026)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.026)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.030)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.030)
              )
            )
          )
          (2): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.035)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.035)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.039)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.039)
              )
            )
          )
          (3): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.043)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.043)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.048)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.048)
              )
            )
          )
          (4): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.052)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.052)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.057)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.057)
              )
            )
          )
          (5): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.061)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.061)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.065)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.065)
              )
            )
          )
          (6): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.070)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.070)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.074)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.074)
              )
            )
          )
          (7): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.078)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.078)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.083)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.083)
              )
            )
          )
          (8): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.087)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.087)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.091)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.091)
              )
            )
          )
        )
        (3): MySequential(
          (0): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.096)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.096)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.100)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.100)
              )
            )
          )
        )
      )
      (avgpool): AdaptiveAvgPool1d(output_size=1)
    )

[wordgrid_embeddings]
WordnnEmbedding(
      (embedding): Embedding(30522, 768)
      (embedding_proj): Linear(in_features=768, out_features=64, bias=False)
    )

[grid_tower]
DaViT(
      (convs): ModuleList(
        (0): ConvEmbed(
          (proj): Conv2d(64, 128, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (1): ConvEmbed(
          (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (2): ConvEmbed(
          (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ConvEmbed(
          (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (blocks): ModuleList(
        (0): MySequential(
          (0): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=128, out_features=384, bias=True)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=128, out_features=512, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=512, out_features=128, bias=True)
                  )
                )
                (drop_path): Identity()
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=128, out_features=384, bias=True)
                  (proj): Linear(in_features=128, out_features=128, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.004)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=128, out_features=512, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=512, out_features=128, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.004)
              )
            )
          )
        )
        (1): MySequential(
          (0): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=256, out_features=768, bias=True)
                  (proj): Linear(in_features=256, out_features=256, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.009)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.009)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=256, out_features=768, bias=True)
                  (proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.013)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.013)
              )
            )
          )
        )
        (2): MySequential(
          (0): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.017)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.017)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.022)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.022)
              )
            )
          )
          (1): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.026)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.026)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.030)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.030)
              )
            )
          )
          (2): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.035)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.035)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.039)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.039)
              )
            )
          )
          (3): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.043)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.043)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.048)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.048)
              )
            )
          )
          (4): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.052)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.052)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.057)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.057)
              )
            )
          )
          (5): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.061)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.061)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.065)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.065)
              )
            )
          )
          (6): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.070)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.070)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.074)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.074)
              )
            )
          )
          (7): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.078)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.078)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.083)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.083)
              )
            )
          )
          (8): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.087)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.087)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=512, out_features=1536, bias=True)
                  (proj): Linear(in_features=512, out_features=512, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.091)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.091)
              )
            )
          )
        )
        (3): MySequential(
          (0): MySequential(
            (spatial_block): SpatialBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                )
              )
              (window_attn): PreNorm(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fn): WindowAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.096)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.096)
              )
            )
            (channel_block): ChannelBlock(
              (conv1): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                )
              )
              (channel_attn): PreNorm(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fn): ChannelAttention(
                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                  (proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (drop_path): DropPath(drop_prob=0.100)
              )
              (conv2): PreNorm(
                (fn): DepthWiseConv2d(
                  (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                )
              )
              (ffn): PreNorm(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (fn): Mlp(
                  (net): Sequential(
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (act): GELU(approximate='none')
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                  )
                )
                (drop_path): DropPath(drop_prob=0.100)
              )
            )
          )
        )
      )
      (norms): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (avgpool): AdaptiveAvgPool1d(output_size=1)
      (head): Linear(in_features=1024, out_features=1000, bias=True)
    )

[image_proj_norm]
LayerNorm((768,), eps=1e-05, elementwise_affine=True)

[image_pos_embed]
LearnedAbsolutePositionEmbedding2D(
      (row_embeddings): Embedding(50, 512)
      (column_embeddings): Embedding(50, 512)
    )

[visual_temporal_embed]
PositionalEmbeddingCosine1D()

[language_model]
Florence2LanguageForConditionalGeneration(
      (model): Florence2LanguageModel(
        (shared): Embedding(51289, 768, padding_idx=1)
        (encoder): Florence2Encoder(
          (embed_tokens): Florence2ScaledWordEmbedding(51289, 768, padding_idx=1)
          (embed_positions): Florence2LearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x Florence2EncoderLayer(
              (self_attn): Florence2SdpaAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (activation_fn): GELUActivation()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (decoder): Florence2Decoder(
          (embed_tokens): Florence2ScaledWordEmbedding(51289, 768, padding_idx=1)
          (embed_positions): Florence2LearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x Florence2DecoderLayer(
              (self_attn): Florence2SdpaAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): GELUActivation()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (encoder_attn): Florence2SdpaAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (lm_head): Linear(in_features=768, out_features=51289, bias=False)
    )




================================================================================
ALL PARAMETERS (name, shape, trainable)
================================================================================

image_projection : [1024, 768] | Trainable: ✅
vision_tower.convs.0.proj.weight : [128, 3, 7, 7] | Trainable: ✅
vision_tower.convs.0.proj.bias : [128] | Trainable: ✅
vision_tower.convs.0.norm.weight : [128] | Trainable: ✅
vision_tower.convs.0.norm.bias : [128] | Trainable: ✅
vision_tower.convs.1.proj.weight : [256, 128, 3, 3] | Trainable: ✅
vision_tower.convs.1.proj.bias : [256] | Trainable: ✅
vision_tower.convs.1.norm.weight : [128] | Trainable: ✅
vision_tower.convs.1.norm.bias : [128] | Trainable: ✅
vision_tower.convs.2.proj.weight : [512, 256, 3, 3] | Trainable: ✅
vision_tower.convs.2.proj.bias : [512] | Trainable: ✅
vision_tower.convs.2.norm.weight : [256] | Trainable: ✅
vision_tower.convs.2.norm.bias : [256] | Trainable: ✅
vision_tower.convs.3.proj.weight : [1024, 512, 3, 3] | Trainable: ✅
vision_tower.convs.3.proj.bias : [1024] | Trainable: ✅
vision_tower.convs.3.norm.weight : [512] | Trainable: ✅
vision_tower.convs.3.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.conv1.fn.dw.weight : [128, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.conv1.fn.dw.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.window_attn.norm.weight : [128] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.window_attn.norm.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.window_attn.fn.qkv.weight : [384, 128] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.window_attn.fn.qkv.bias : [384] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.window_attn.fn.proj.weight : [128, 128] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.window_attn.fn.proj.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.conv2.fn.dw.weight : [128, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.conv2.fn.dw.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.ffn.norm.weight : [128] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.ffn.norm.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc1.weight : [512, 128] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc1.bias : [512] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc2.weight : [128, 512] | Trainable: ✅
vision_tower.blocks.0.0.spatial_block.ffn.fn.net.fc2.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.conv1.fn.dw.weight : [128, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.conv1.fn.dw.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.channel_attn.norm.weight : [128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.channel_attn.norm.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.channel_attn.fn.qkv.weight : [384, 128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.channel_attn.fn.qkv.bias : [384] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.channel_attn.fn.proj.weight : [128, 128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.channel_attn.fn.proj.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.conv2.fn.dw.weight : [128, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.conv2.fn.dw.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.ffn.norm.weight : [128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.ffn.norm.bias : [128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc1.weight : [512, 128] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc1.bias : [512] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc2.weight : [128, 512] | Trainable: ✅
vision_tower.blocks.0.0.channel_block.ffn.fn.net.fc2.bias : [128] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.conv1.fn.dw.weight : [256, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.conv1.fn.dw.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.window_attn.norm.weight : [256] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.window_attn.norm.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.window_attn.fn.qkv.weight : [768, 256] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.window_attn.fn.qkv.bias : [768] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.window_attn.fn.proj.weight : [256, 256] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.window_attn.fn.proj.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.conv2.fn.dw.weight : [256, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.conv2.fn.dw.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.ffn.norm.weight : [256] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.ffn.norm.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc1.weight : [1024, 256] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc1.bias : [1024] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc2.weight : [256, 1024] | Trainable: ✅
vision_tower.blocks.1.0.spatial_block.ffn.fn.net.fc2.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.conv1.fn.dw.weight : [256, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.conv1.fn.dw.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.channel_attn.norm.weight : [256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.channel_attn.norm.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.channel_attn.fn.qkv.weight : [768, 256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.channel_attn.fn.qkv.bias : [768] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.channel_attn.fn.proj.weight : [256, 256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.channel_attn.fn.proj.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.conv2.fn.dw.weight : [256, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.conv2.fn.dw.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.ffn.norm.weight : [256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.ffn.norm.bias : [256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc1.weight : [1024, 256] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc1.bias : [1024] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc2.weight : [256, 1024] | Trainable: ✅
vision_tower.blocks.1.0.channel_block.ffn.fn.net.fc2.bias : [256] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.0.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.0.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.1.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.1.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.2.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.2.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.3.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.3.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.4.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.4.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.5.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.5.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.6.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.6.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.7.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.7.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.8.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.ffn.norm.weight : [512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.ffn.norm.bias : [512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
vision_tower.blocks.2.8.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.conv1.fn.dw.weight : [1024, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.conv1.fn.dw.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.window_attn.norm.weight : [1024] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.window_attn.norm.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.window_attn.fn.qkv.weight : [3072, 1024] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.window_attn.fn.qkv.bias : [3072] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.window_attn.fn.proj.weight : [1024, 1024] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.window_attn.fn.proj.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.conv2.fn.dw.weight : [1024, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.conv2.fn.dw.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.ffn.norm.weight : [1024] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.ffn.norm.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc1.weight : [4096, 1024] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc1.bias : [4096] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc2.weight : [1024, 4096] | Trainable: ✅
vision_tower.blocks.3.0.spatial_block.ffn.fn.net.fc2.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.conv1.fn.dw.weight : [1024, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.conv1.fn.dw.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.channel_attn.norm.weight : [1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.channel_attn.norm.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.channel_attn.fn.qkv.weight : [3072, 1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.channel_attn.fn.qkv.bias : [3072] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.channel_attn.fn.proj.weight : [1024, 1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.channel_attn.fn.proj.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.conv2.fn.dw.weight : [1024, 1, 3, 3] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.conv2.fn.dw.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.ffn.norm.weight : [1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.ffn.norm.bias : [1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc1.weight : [4096, 1024] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc1.bias : [4096] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc2.weight : [1024, 4096] | Trainable: ✅
vision_tower.blocks.3.0.channel_block.ffn.fn.net.fc2.bias : [1024] | Trainable: ✅
wordgrid_embeddings.embedding.weight : [30522, 768] | Trainable: ❌
wordgrid_embeddings.embedding_proj.weight : [64, 768] | Trainable: ✅
grid_tower.convs.0.proj.weight : [128, 64, 7, 7] | Trainable: ✅
grid_tower.convs.0.proj.bias : [128] | Trainable: ✅
grid_tower.convs.0.norm.weight : [128] | Trainable: ✅
grid_tower.convs.0.norm.bias : [128] | Trainable: ✅
grid_tower.convs.1.proj.weight : [256, 128, 3, 3] | Trainable: ✅
grid_tower.convs.1.proj.bias : [256] | Trainable: ✅
grid_tower.convs.1.norm.weight : [128] | Trainable: ✅
grid_tower.convs.1.norm.bias : [128] | Trainable: ✅
grid_tower.convs.2.proj.weight : [512, 256, 3, 3] | Trainable: ✅
grid_tower.convs.2.proj.bias : [512] | Trainable: ✅
grid_tower.convs.2.norm.weight : [256] | Trainable: ✅
grid_tower.convs.2.norm.bias : [256] | Trainable: ✅
grid_tower.convs.3.proj.weight : [1024, 512, 3, 3] | Trainable: ✅
grid_tower.convs.3.proj.bias : [1024] | Trainable: ✅
grid_tower.convs.3.norm.weight : [512] | Trainable: ✅
grid_tower.convs.3.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.conv1.fn.dw.weight : [128, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.conv1.fn.dw.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.window_attn.norm.weight : [128] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.window_attn.norm.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.window_attn.fn.qkv.weight : [384, 128] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.window_attn.fn.qkv.bias : [384] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.window_attn.fn.proj.weight : [128, 128] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.window_attn.fn.proj.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.conv2.fn.dw.weight : [128, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.conv2.fn.dw.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.ffn.norm.weight : [128] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.ffn.norm.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.ffn.fn.net.fc1.weight : [512, 128] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.ffn.fn.net.fc1.bias : [512] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.ffn.fn.net.fc2.weight : [128, 512] | Trainable: ✅
grid_tower.blocks.0.0.spatial_block.ffn.fn.net.fc2.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.conv1.fn.dw.weight : [128, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.conv1.fn.dw.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.channel_attn.norm.weight : [128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.channel_attn.norm.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.channel_attn.fn.qkv.weight : [384, 128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.channel_attn.fn.qkv.bias : [384] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.channel_attn.fn.proj.weight : [128, 128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.channel_attn.fn.proj.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.conv2.fn.dw.weight : [128, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.conv2.fn.dw.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.ffn.norm.weight : [128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.ffn.norm.bias : [128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.ffn.fn.net.fc1.weight : [512, 128] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.ffn.fn.net.fc1.bias : [512] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.ffn.fn.net.fc2.weight : [128, 512] | Trainable: ✅
grid_tower.blocks.0.0.channel_block.ffn.fn.net.fc2.bias : [128] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.conv1.fn.dw.weight : [256, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.conv1.fn.dw.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.window_attn.norm.weight : [256] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.window_attn.norm.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.window_attn.fn.qkv.weight : [768, 256] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.window_attn.fn.qkv.bias : [768] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.window_attn.fn.proj.weight : [256, 256] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.window_attn.fn.proj.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.conv2.fn.dw.weight : [256, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.conv2.fn.dw.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.ffn.norm.weight : [256] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.ffn.norm.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.ffn.fn.net.fc1.weight : [1024, 256] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.ffn.fn.net.fc1.bias : [1024] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.ffn.fn.net.fc2.weight : [256, 1024] | Trainable: ✅
grid_tower.blocks.1.0.spatial_block.ffn.fn.net.fc2.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.conv1.fn.dw.weight : [256, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.conv1.fn.dw.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.channel_attn.norm.weight : [256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.channel_attn.norm.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.channel_attn.fn.qkv.weight : [768, 256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.channel_attn.fn.qkv.bias : [768] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.channel_attn.fn.proj.weight : [256, 256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.channel_attn.fn.proj.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.conv2.fn.dw.weight : [256, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.conv2.fn.dw.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.ffn.norm.weight : [256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.ffn.norm.bias : [256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.ffn.fn.net.fc1.weight : [1024, 256] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.ffn.fn.net.fc1.bias : [1024] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.ffn.fn.net.fc2.weight : [256, 1024] | Trainable: ✅
grid_tower.blocks.1.0.channel_block.ffn.fn.net.fc2.bias : [256] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.0.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.0.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.1.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.1.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.2.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.2.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.3.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.3.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.4.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.4.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.5.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.5.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.6.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.6.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.7.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.7.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.window_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.window_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.window_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.window_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.window_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.window_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.8.spatial_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.conv1.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.conv1.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.channel_attn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.channel_attn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.channel_attn.fn.qkv.weight : [1536, 512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.channel_attn.fn.qkv.bias : [1536] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.channel_attn.fn.proj.weight : [512, 512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.channel_attn.fn.proj.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.conv2.fn.dw.weight : [512, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.conv2.fn.dw.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.ffn.norm.weight : [512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.ffn.norm.bias : [512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.ffn.fn.net.fc1.weight : [2048, 512] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.ffn.fn.net.fc1.bias : [2048] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.ffn.fn.net.fc2.weight : [512, 2048] | Trainable: ✅
grid_tower.blocks.2.8.channel_block.ffn.fn.net.fc2.bias : [512] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.conv1.fn.dw.weight : [1024, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.conv1.fn.dw.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.window_attn.norm.weight : [1024] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.window_attn.norm.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.window_attn.fn.qkv.weight : [3072, 1024] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.window_attn.fn.qkv.bias : [3072] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.window_attn.fn.proj.weight : [1024, 1024] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.window_attn.fn.proj.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.conv2.fn.dw.weight : [1024, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.conv2.fn.dw.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.ffn.norm.weight : [1024] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.ffn.norm.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.ffn.fn.net.fc1.weight : [4096, 1024] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.ffn.fn.net.fc1.bias : [4096] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.ffn.fn.net.fc2.weight : [1024, 4096] | Trainable: ✅
grid_tower.blocks.3.0.spatial_block.ffn.fn.net.fc2.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.conv1.fn.dw.weight : [1024, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.conv1.fn.dw.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.channel_attn.norm.weight : [1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.channel_attn.norm.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.channel_attn.fn.qkv.weight : [3072, 1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.channel_attn.fn.qkv.bias : [3072] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.channel_attn.fn.proj.weight : [1024, 1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.channel_attn.fn.proj.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.conv2.fn.dw.weight : [1024, 1, 3, 3] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.conv2.fn.dw.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.ffn.norm.weight : [1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.ffn.norm.bias : [1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.ffn.fn.net.fc1.weight : [4096, 1024] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.ffn.fn.net.fc1.bias : [4096] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.ffn.fn.net.fc2.weight : [1024, 4096] | Trainable: ✅
grid_tower.blocks.3.0.channel_block.ffn.fn.net.fc2.bias : [1024] | Trainable: ✅
grid_tower.norms.weight : [1024] | Trainable: ✅
grid_tower.norms.bias : [1024] | Trainable: ✅
grid_tower.head.weight : [1000, 1024] | Trainable: ✅
grid_tower.head.bias : [1000] | Trainable: ✅
image_proj_norm.weight : [768] | Trainable: ✅
image_proj_norm.bias : [768] | Trainable: ✅
image_pos_embed.row_embeddings.weight : [50, 512] | Trainable: ✅
image_pos_embed.column_embeddings.weight : [50, 512] | Trainable: ✅
language_model.model.shared.weight : [51289, 768] | Trainable: ✅
language_model.model.encoder.embed_positions.weight : [1026, 768] | Trainable: ✅
language_model.model.encoder.layers.0.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.0.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.0.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.0.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.0.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.0.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.0.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.0.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.0.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.0.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.0.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.encoder.layers.0.fc1.bias : [3072] | Trainable: ✅
language_model.model.encoder.layers.0.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.encoder.layers.0.fc2.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.0.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.0.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.1.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.1.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.1.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.1.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.1.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.1.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.1.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.1.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.1.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.1.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.1.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.encoder.layers.1.fc1.bias : [3072] | Trainable: ✅
language_model.model.encoder.layers.1.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.encoder.layers.1.fc2.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.1.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.1.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.2.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.2.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.2.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.2.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.2.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.2.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.2.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.2.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.2.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.2.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.2.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.encoder.layers.2.fc1.bias : [3072] | Trainable: ✅
language_model.model.encoder.layers.2.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.encoder.layers.2.fc2.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.2.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.2.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.3.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.3.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.3.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.3.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.3.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.3.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.3.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.3.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.3.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.3.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.3.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.encoder.layers.3.fc1.bias : [3072] | Trainable: ✅
language_model.model.encoder.layers.3.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.encoder.layers.3.fc2.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.3.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.3.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.4.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.4.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.4.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.4.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.4.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.4.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.4.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.4.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.4.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.4.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.4.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.encoder.layers.4.fc1.bias : [3072] | Trainable: ✅
language_model.model.encoder.layers.4.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.encoder.layers.4.fc2.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.4.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.4.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.5.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.5.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.5.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.5.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.5.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.5.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.5.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.encoder.layers.5.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.5.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.5.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.5.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.encoder.layers.5.fc1.bias : [3072] | Trainable: ✅
language_model.model.encoder.layers.5.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.encoder.layers.5.fc2.bias : [768] | Trainable: ✅
language_model.model.encoder.layers.5.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.encoder.layers.5.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.encoder.layernorm_embedding.weight : [768] | Trainable: ✅
language_model.model.encoder.layernorm_embedding.bias : [768] | Trainable: ✅
language_model.model.decoder.embed_positions.weight : [1026, 768] | Trainable: ✅
language_model.model.decoder.layers.0.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.0.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.0.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.0.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.0.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.0.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.encoder_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.0.encoder_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.encoder_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.0.encoder_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.encoder_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.0.encoder_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.encoder_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.0.encoder_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.encoder_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.0.encoder_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.decoder.layers.0.fc1.bias : [3072] | Trainable: ✅
language_model.model.decoder.layers.0.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.decoder.layers.0.fc2.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.0.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.0.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.1.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.1.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.1.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.1.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.1.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.encoder_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.1.encoder_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.encoder_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.1.encoder_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.encoder_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.1.encoder_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.encoder_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.1.encoder_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.encoder_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.1.encoder_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.decoder.layers.1.fc1.bias : [3072] | Trainable: ✅
language_model.model.decoder.layers.1.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.decoder.layers.1.fc2.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.1.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.1.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.2.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.2.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.2.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.2.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.2.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.encoder_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.2.encoder_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.encoder_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.2.encoder_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.encoder_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.2.encoder_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.encoder_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.2.encoder_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.encoder_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.2.encoder_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.decoder.layers.2.fc1.bias : [3072] | Trainable: ✅
language_model.model.decoder.layers.2.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.decoder.layers.2.fc2.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.2.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.2.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.3.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.3.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.3.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.3.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.3.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.encoder_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.3.encoder_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.encoder_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.3.encoder_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.encoder_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.3.encoder_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.encoder_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.3.encoder_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.encoder_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.3.encoder_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.decoder.layers.3.fc1.bias : [3072] | Trainable: ✅
language_model.model.decoder.layers.3.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.decoder.layers.3.fc2.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.3.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.3.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.4.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.4.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.4.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.4.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.4.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.encoder_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.4.encoder_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.encoder_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.4.encoder_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.encoder_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.4.encoder_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.encoder_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.4.encoder_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.encoder_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.4.encoder_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.decoder.layers.4.fc1.bias : [3072] | Trainable: ✅
language_model.model.decoder.layers.4.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.decoder.layers.4.fc2.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.4.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.4.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.self_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.5.self_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.self_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.5.self_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.self_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.5.self_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.self_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.5.self_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.self_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.5.self_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.encoder_attn.k_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.5.encoder_attn.k_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.encoder_attn.v_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.5.encoder_attn.v_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.encoder_attn.q_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.5.encoder_attn.q_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.encoder_attn.out_proj.weight : [768, 768] | Trainable: ✅
language_model.model.decoder.layers.5.encoder_attn.out_proj.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.encoder_attn_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.5.encoder_attn_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.fc1.weight : [3072, 768] | Trainable: ✅
language_model.model.decoder.layers.5.fc1.bias : [3072] | Trainable: ✅
language_model.model.decoder.layers.5.fc2.weight : [768, 3072] | Trainable: ✅
language_model.model.decoder.layers.5.fc2.bias : [768] | Trainable: ✅
language_model.model.decoder.layers.5.final_layer_norm.weight : [768] | Trainable: ✅
language_model.model.decoder.layers.5.final_layer_norm.bias : [768] | Trainable: ✅
language_model.model.decoder.layernorm_embedding.weight : [768] | Trainable: ✅
language_model.model.decoder.layernorm_embedding.bias : [768] | Trainable: ✅
language_model.lm_head.weight : [51289, 768] | Trainable: ✅


