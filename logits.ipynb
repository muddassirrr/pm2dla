{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_MODEL_CARD = \"thewalnutaisg/florence2-large-doclaynet-70k\"\n",
    "STUDENT_MODEL_CARD = \"microsoft/Florence-2-base-ft\"\n",
    "DATASET_CARD = \"katphlab/doclaynet-table\"\n",
    "PROMPT = \"<OD>\"\n",
    "IGNORE_ID = -100  # Pytorch ignore index when computing loss\n",
    "MAX_LENGTH = 512\n",
    "DATA_SPLIT = \"val\"\n",
    "RUN_NAME = \"distillv2\"\n",
    "OUTPUT_DIR = \"./runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models and processor\n",
    "processor = AutoProcessor.from_pretrained(STUDENT_MODEL_CARD, trust_remote_code=True)\n",
    "config = AutoConfig.from_pretrained(TEACHER_MODEL_CARD, trust_remote_code=True)\n",
    "config.vision_config.model_type = \"davit\"\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TEACHER_MODEL_CARD, trust_remote_code=True, config=config\n",
    ")\n",
    "# Move models to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model.to(device)\n",
    "\n",
    "# Freeze the teacher model\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO dataset\n",
    "dataset = (\n",
    "    load_dataset(DATASET_CARD, DATA_SPLIT, split=DATA_SPLIT, num_proc=12)\n",
    "    .shuffle(seed=42)\n",
    "    .select(range(10))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "def preprocess_function(examples):\n",
    "    prompt_texts = [PROMPT] * len(examples[\"image\"])\n",
    "\n",
    "    inputs = processor(\n",
    "        images=examples[\"image\"],\n",
    "        text=prompt_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    labels = processor.tokenizer(\n",
    "        examples[\"bbox_str\"],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_token_type_ids=False,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = IGNORE_ID\n",
    "    # No need to remove batch dimension as we're processing in batches\n",
    "    inputs[\"labels\"] = labels\n",
    "\n",
    "    # Move all inputs to CUDA\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Compute teacher logits\n",
    "    with torch.no_grad():\n",
    "        teacher_model.eval()\n",
    "        teacher_outputs = teacher_model(**inputs)\n",
    "        teacher_logits = teacher_outputs.logits\n",
    "\n",
    "    examples[\"teacher_logits\"] = teacher_logits.cpu()\n",
    "\n",
    "    for key, value in inputs.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            inputs[key] = value.tolist()\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_dataset = dataset.map(preprocess_function, batched=True, batch_size=2)\n",
    "logit_dataset.save_to_disk(\"./data/teacher_logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize top k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cpm_for_k(logits, k_values):\n",
    "    \"\"\"\n",
    "    Compute the Cumulative Probability Mass (CPM) for different k values.\n",
    "\n",
    "    Args:\n",
    "    logits (torch.Tensor): The full logits from the teacher model.\n",
    "    k_values (list): List of k values to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with k values as keys and mean CPM as values.\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    sorted_probs, _ = torch.sort(probs, dim=-1, descending=True)\n",
    "\n",
    "    cpm_dict = {}\n",
    "    for k in k_values:\n",
    "        top_k_probs = sorted_probs[:, :, :k]\n",
    "        cpm = top_k_probs.sum(dim=-1).mean().item()\n",
    "        cpm_dict[k] = cpm\n",
    "\n",
    "    return cpm_dict\n",
    "\n",
    "\n",
    "def select_optimal_k(cpm_dict, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Select the smallest k that achieves a CPM above the threshold.\n",
    "\n",
    "    Args:\n",
    "    cpm_dict (dict): Dictionary with k values as keys and mean CPM as values.\n",
    "    threshold (float): The desired CPM threshold.\n",
    "\n",
    "    Returns:\n",
    "    int: The optimal k value.\n",
    "    \"\"\"\n",
    "    for k, cpm in sorted(cpm_dict.items()):\n",
    "        if cpm >= threshold:\n",
    "            return k\n",
    "    return max(cpm_dict.keys())  # If no k meets the threshold, return the largest k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_logits_from_topk(top_k_logits, top_k_indices, vocab_size):\n",
    "    # Convert to tensors if they're lists\n",
    "    top_k_logits = (\n",
    "        torch.tensor(top_k_logits) if isinstance(top_k_logits, list) else top_k_logits\n",
    "    )\n",
    "    top_k_indices = (\n",
    "        torch.tensor(top_k_indices)\n",
    "        if isinstance(top_k_indices, list)\n",
    "        else top_k_indices\n",
    "    )\n",
    "\n",
    "    # Ensure tensors are of type float and long respectively\n",
    "    top_k_logits = top_k_logits.float()\n",
    "    top_k_indices = top_k_indices.long()\n",
    "\n",
    "    # Get the shape of the original logits\n",
    "    seq_len, k = top_k_indices.shape\n",
    "\n",
    "    # Create a tensor with the default logit value\n",
    "    reconstructed_logits = torch.full((seq_len, vocab_size), 0.0)\n",
    "\n",
    "    # Use scatter to place the top-k logits in the correct positions\n",
    "    reconstructed_logits.scatter_(-1, top_k_indices, top_k_logits)\n",
    "\n",
    "    return reconstructed_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cpm(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    sorted_probs, _ = torch.sort(probs, dim=-1, descending=True)\n",
    "    cpm = torch.cumsum(sorted_probs, dim=-1)\n",
    "    return cpm\n",
    "\n",
    "\n",
    "def plot_cpm_comparison(original_cpm, reconstructed_cpm, k):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(original_cpm.mean(dim=0).cpu().numpy(), label=\"Original\")\n",
    "    plt.plot(reconstructed_cpm.mean(dim=0).cpu().numpy(), label=\"Reconstructed\")\n",
    "    plt.axvline(x=k, color=\"r\", linestyle=\"--\", label=f\"k={k}\")\n",
    "    plt.xlabel(\"Top-k\")\n",
    "    plt.ylabel(\"Cumulative Probability Mass\")\n",
    "    plt.title(\"Comparison of Original and Reconstructed CPM\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_logits_heatmap(original_logits, reconstructed_logits, k):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # Ensure logits are 2D\n",
    "    if original_logits.dim() == 1:\n",
    "        original_logits = original_logits.unsqueeze(0)\n",
    "    if reconstructed_logits.dim() == 1:\n",
    "        reconstructed_logits = reconstructed_logits.unsqueeze(0)\n",
    "\n",
    "    # Limit to top-k values\n",
    "    top_k_original = torch.topk(\n",
    "        original_logits, min(k, original_logits.size(-1)), dim=-1\n",
    "    ).values\n",
    "    top_k_reconstructed = torch.topk(\n",
    "        reconstructed_logits, min(k, reconstructed_logits.size(-1)), dim=-1\n",
    "    ).values\n",
    "\n",
    "    sns.heatmap(top_k_original.cpu().numpy(), ax=ax1, cmap=\"viridis\")\n",
    "    ax1.set_title(\"Original Logits (Top-k)\")\n",
    "    ax1.set_xlabel(\"Token Index\")\n",
    "    ax1.set_ylabel(\"Sequence Position\" if original_logits.dim() > 1 else \"Top-k Values\")\n",
    "\n",
    "    sns.heatmap(top_k_reconstructed.cpu().numpy(), ax=ax2, cmap=\"viridis\")\n",
    "    ax2.set_title(\"Reconstructed Logits (Top-k)\")\n",
    "    ax2.set_xlabel(\"Token Index\")\n",
    "    ax2.set_ylabel(\n",
    "        \"Sequence Position\" if reconstructed_logits.dim() > 1 else \"Top-k Values\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_cpm_and_logits(original_logits, top_k_indices, top_k_logits, k):\n",
    "    # Reconstruct logits\n",
    "    vocab_size = original_logits.size(-1)\n",
    "    reconstructed_logits = reconstruct_logits_from_topk(\n",
    "        top_k_logits, top_k_indices, vocab_size\n",
    "    )\n",
    "\n",
    "    # Compute CPM\n",
    "    original_cpm = compute_cpm(original_logits)\n",
    "    reconstructed_cpm = compute_cpm(reconstructed_logits)\n",
    "\n",
    "    # Plot CPM comparison\n",
    "    plot_cpm_comparison(original_cpm, reconstructed_cpm, k)\n",
    "\n",
    "    # Plot logits heatmap\n",
    "    plot_logits_heatmap(original_logits[0], reconstructed_logits[0], k)\n",
    "\n",
    "    # Print statistics\n",
    "    loss = F.kl_div(\n",
    "        F.log_softmax(original_logits, dim=-1),\n",
    "        F.log_softmax(reconstructed_logits, dim=-1),\n",
    "        reduction=\"batchmean\",\n",
    "        log_target=True,\n",
    "    )\n",
    "    print(f\"Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_dataset = load_from_disk(\"./data/teacher_logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top-k predictions\n",
    "teacher_logits = torch.tensor(logit_dataset[0][\"teacher_logits\"])\n",
    "k = 10  # You can adjust this value\n",
    "top_k_logits, top_k_indices = torch.topk(teacher_logits, k, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cpm_and_logits(\n",
    "    teacher_logits, top_k_indices, top_k_logits, k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
